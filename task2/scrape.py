"""import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv


BASE_URL = "https://ru.wikipedia.org"

url = "https://ru.wikipedia.org/w/index.php?title=%D0%9A%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D1%8F:%D0%96%D0%B8%D0%B2%D0%BE%D1%82%D0%BD%D1%8B%D0%B5_%D0%BF%D0%BE_%D0%B0%D0%BB%D1%84%D0%B0%D0%B2%D0%B8%D1%82%D1%83&from=%3Cb%3E%D0%90%3C%2Fb%3E"


def get_links_from_page(url):
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    links_div = soup.find('div', class_='mw-category mw-category-columns')
    next_link_div = soup.find('div', class_='mw-category-generated')

    links = []
    next_absolute_url = None

    if links_div:
        a_tags = links_div.find_all('a')
        links = [a.get('href') for a in a_tags if a.get('href')]

    if next_link_div:
        next_link_tag = next_link_div.find('a', string=lambda text: text and '–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞' in text)
        if next_link_tag:
            next_relative_path = next_link_tag.get('href')
            next_absolute_url = urljoin(BASE_URL, next_relative_path)
            print(f"‚û°Ô∏è Next page found: {next_absolute_url}")
        else:
            print("‚õî No next page link found.")
    else:
        print("‚õî No <div class='mw-pages'> found on the page.")

    return links, next_absolute_url

def get_all_links(start_url):
    current_url = start_url
    all_links = []
    page_count = 0

    while current_url:
        print(f"üîÑ Parsing page {page_count + 1}: {current_url}")
        links, next_url = get_links_from_page(current_url)
        all_links.extend(links)
        current_url = next_url
        page_count += 1

    print(f"‚úÖ Finished! Parsed {page_count} pages.")
    
    letter_count = {}
    for link in all_links:
        name = link.split('/')[-1]
        if name:
            name = requests.utils.unquote(name)
            first_letter = name[0].upper()
            if first_letter.isalpha():
                letter_count[first_letter] = letter_count.get(first_letter, 0) + 1

    return letter_count


def save_counts_to_csv(counts_dict, filename='beasts.csv'):
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Letter', 'Count'])  # header
        for letter, count in sorted(counts_dict.items()):
            writer.writerow([letter, count])

letter_counts = get_all_links(url)
save_counts_to_csv(letter_counts)
print(f"\n‚úÖ Done! Letter frequency saved to beasts.csv")
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv

# –ë–∞–∑–æ–≤—ã–π URL –í–∏–∫–∏–ø–µ–¥–∏–∏
BASE_URL = "https://ru.wikipedia.org"

# –ù–∞—á–∞–ª—å–Ω—ã–π URL ‚Äî –∫–∞—Ç–µ–≥–æ—Ä–∏—è "–ñ–∏–≤–æ—Ç–Ω—ã–µ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É", –Ω–∞—á–∏–Ω–∞—è —Å "–ê"
url = "https://ru.wikipedia.org/w/index.php?title=%D0%9A%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D1%8F:%D0%96%D0%B8%D0%B2%D0%BE%D1%82%D0%BD%D1%8B%D0%B5_%D0%BF%D0%BE_%D0%B0%D0%BB%D1%84%D0%B0%D0%B2%D0%B8%D1%82%D1%83&from=%3Cb%3E%D0%90%3C%2Fb%3E"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
def get_links_from_page(url):
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    links_div = soup.find('div', class_='mw-category mw-category-columns')
    next_link_div = soup.find('div', class_='mw-category-generated')

    links = []
    next_absolute_url = None

    if links_div:
        a_tags = links_div.find_all('a')
        links = [a.get('href') for a in a_tags if a.get('href')]

    if next_link_div:
        next_link_tag = next_link_div.find('a', string=lambda text: text and '–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞' in text)
        if next_link_tag:
            next_relative_path = next_link_tag.get('href')
            next_absolute_url = urljoin(BASE_URL, next_relative_path)
            print(f"‚û°Ô∏è –ù–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {next_absolute_url}")
        else:
            print("‚õî –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    else:
        print("‚õî –ë–ª–æ–∫ <div class='mw-pages'> –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")

    return links, next_absolute_url

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
def get_all_links(start_url):
    current_url = start_url
    all_links = []
    page_count = 0

    while current_url:
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_count + 1}: {current_url}")
        links, next_url = get_links_from_page(current_url)
        all_links.extend(links)
        current_url = next_url
        page_count += 1

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {page_count} —Å—Ç—Ä–∞–Ω–∏—Ü.")
    
    # –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∂–∏–≤–æ—Ç–Ω—ã—Ö –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ
    letter_count = {}
    for link in all_links:
        name = link.split('/')[-1]
        if name:
            name = requests.utils.unquote(name)
            first_letter = name[0].upper()
            if first_letter.isalpha():
                letter_count[first_letter] = letter_count.get(first_letter, 0) + 1

    return letter_count

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV-—Ñ–∞–π–ª
def save_counts_to_csv(counts_dict, filename='task2/beasts.csv'):
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['–ë—É–∫–≤–∞', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'])  # –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–æ–≤
        for letter, count in sorted(counts_dict.items()):
            writer.writerow([letter, count])

# –ó–∞–ø—É—Å–∫
letter_counts = get_all_links(url)
save_counts_to_csv(letter_counts)
print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ß–∞—Å—Ç–æ—Ç–∞ –±—É–∫–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª beasts.csv")
